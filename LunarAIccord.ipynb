{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! python extract_ossc_formulas.py \\\n",
    "  --sheet-id 1U4XKFcUypBs0ruJybO9oXKNgIs6_fUDATj4U7gjuZr4 \\\n",
    "  --gid 554372652 \\\n",
    "  --out-dir out/ \\\n",
    "  --source \"Open Source Smell Culture – Formulas Database\" \\\n",
    "  --source-url \"https://docs.google.com/spreadsheets/d/1U4XKFcUypBs0ruJybO9oXKNgIs6_fUDATj4U7gjuZr4/edit?gid=554372652#gid=554372652\" \\\n",
    "  --license \"CC BY-SA 4.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./opt/anaconda3/lib/python3.9/site-packages (1.4.4)\n",
      "Requirement already satisfied: requests in ./opt/anaconda3/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: openpyxl in ./opt/anaconda3/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./opt/anaconda3/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./opt/anaconda3/lib/python3.9/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./opt/anaconda3/lib/python3.9/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: et-xmlfile in ./opt/anaconda3/lib/python3.9/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! python Desktop/LunarAIccord/helpers/extract_ossc_formulas.py \\\n",
    "  --sheet-id 1U4XKFcUypBs0ruJybO9oXKNgIs6_fUDATj4U7gjuZr4 \\\n",
    "  --gid 554372652 \\\n",
    "  --out-dir /Users/ranykhirbawi/Desktop/my_csv.csv \\\n",
    "  --source \"Open Source Smell Culture – Formulas Database\" \\\n",
    "  --source-url \"https://docs.google.com/spreadsheets/d/1U4XKFcUypBs0ruJybO9oXKNgIs6_fUDATj4U7gjuZr4/edit?gid=554372652#gid=554372652\" \\\n",
    "  --license \"CC BY-SA 4.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1 formulas to:\n",
      " - /Users/ranykhirbawi/Desktop/LunarAIccord/formulas.json\n",
      " - /Users/ranykhirbawi/Desktop/LunarAIccord/formulas.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python Desktop/LunarAIccord/helpers/extract_ossc_formulas.py \\\n",
    "  --sheet-id 1U4XKFcUypBs0ruJybO9oXKNgIs6_fUDATj4U7gjuZr4 \\\n",
    "  --gid 554372652 \\\n",
    "  --out-dir /Users/ranykhirbawi/Desktop/LunarAIccord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8977 <!DOCTYPE html><style nonce=\"-y2lYacG5ONCM85kEW5law\">body{height:100%;margin:0;width:100%}@media (max-height:350px){.button{font-size:10px}.button-container{margin-top:16px}.button.primary-button,.but\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "u=\"https://docs.google.com/spreadsheets/d/1U4XKFcUypBs0ruJybO9oXKNgIs6_fUDATj4U7gjuZr4/export?format=csv&gid=554372652\"\n",
    "t=requests.get(u,headers={'User-Agent':'Mozilla/5.0'}).text\n",
    "print(len(t), t[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [markdown]\n",
    "# # Lunar AIccord — offline MVP scaffold (writes files to ./lunar-aiccord)\n",
    "# # Requirements: Python 3.11+, pandas, numpy (no external APIs needed)\n",
    "\n",
    "# # %%\n",
    "# import os, textwrap, shutil, json\n",
    "# from pathlib import Path\n",
    "\n",
    "# BASE = Path(\"/Users/ranykhirbawi/Desktop/LunarAIccord\")\n",
    "# AGENT = BASE/\"agent\"\n",
    "# INGEST = BASE/\"data_ingest\"\n",
    "# EXAMPLES = BASE/\"examples\"\n",
    "# OUTPUTS = EXAMPLES/\"outputs\"\n",
    "# INDICES = BASE/\"indices\"\n",
    "# TOKENS = BASE/\"tokens_count\"\n",
    "\n",
    "# # Clean slate (CAREFUL if you modified files locally)\n",
    "# if BASE.exists():\n",
    "#     shutil.rmtree(BASE)\n",
    "# for p in (AGENT, INGEST, EXAMPLES, OUTPUTS, INDICES, TOKENS):\n",
    "#     p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# def w(path: Path, content: str):\n",
    "#     path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#     path.write_text(textwrap.dedent(content).strip()+\"\\n\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ranykhirbawi/Desktop/LunarAIccord/agent\n"
     ]
    }
   ],
   "source": [
    "print(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote files to /Users/ranykhirbawi/Desktop/LunarAIccord/data\n",
      "Now run Cell 2 to build indices and Cell 3 for a demo.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------- core metadata files ----------------\n",
    "w(BASE/\"requirements.txt\", \"\"\"\n",
    "pandas\n",
    "numpy\n",
    "python-dotenv\n",
    "langchain\n",
    "\"\"\")\n",
    "\n",
    "w(BASE/\".env.example\", \"API_KEY=\\n\")\n",
    "\n",
    "w(EXAMPLES/\"01_brief.txt\", \"\"\"\n",
    "Create a tranquil summer lobby signature for a spa brand. \n",
    "Use calm, airy vibes; hints of sea breeze. The brand colors are teal and sand.\n",
    "\"\"\")\n",
    "w(EXAMPLES/\"02_brief.txt\", \"\"\"\n",
    "A winter scent for a luxury menswear boutique: confident, polished, evening wear.\n",
    "Prefer warm woods and a touch of leather. Brand colors: black and gold.\n",
    "\"\"\")\n",
    "w(EXAMPLES/\"03_brief.txt\", \"\"\"\n",
    "Fresh, clean fragrance for a wellness app launch. Minimalist, green, uplifting.\n",
    "Avoid heavy animalic notes. Colors: soft green and white.\n",
    "\"\"\")\n",
    "w(TOKENS/\"total_tokens.txt\", \"0\")\n",
    "\n",
    "# ---------------- agent/local_utils.py ----------------\n",
    "w(AGENT/\"local_utils.py\", r\"\"\"\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9#]+\", \" \", s)\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def tokenize(s: str):\n",
    "    s = normalize_text(s)\n",
    "    return [t for t in s.split() if t]\n",
    "\n",
    "class TinyTfidf:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.idf = []\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        docs = [tokenize(x) for x in corpus]\n",
    "        vocab = {}\n",
    "        for doc in docs:\n",
    "            for t in set(doc):\n",
    "                if t not in vocab:\n",
    "                    vocab[t] = len(vocab)\n",
    "        N = len(docs)\n",
    "        df = [0]*len(vocab)\n",
    "        for doc in docs:\n",
    "            seen = set(doc)\n",
    "            for t in seen:\n",
    "                df[vocab[t]] += 1\n",
    "        idf = [math.log((N+1)/(dfi+1)) + 1.0 for dfi in df]\n",
    "        self.vocab = vocab\n",
    "        self.idf = idf\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"TinyTfidf not fitted\")\n",
    "        X = []\n",
    "        for text in corpus:\n",
    "            tokens = tokenize(text)\n",
    "            counts = Counter(tokens)\n",
    "            vec = [0.0]*len(self.vocab)\n",
    "            for t, c in counts.items():\n",
    "                j = self.vocab.get(t)\n",
    "                if j is not None:\n",
    "                    vec[j] = (c) * self.idf[j]\n",
    "            norm = math.sqrt(sum(v*v for v in vec)) or 1.0\n",
    "            vec = [v/norm for v in vec]\n",
    "            X.append(vec)\n",
    "        return X\n",
    "\n",
    "def cosine(u, v):\n",
    "    return sum(a*b for a,b in zip(u,v))\n",
    "\n",
    "def flatten_json(obj, prefix=\"\"):\n",
    "    parts = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            parts.append(f\"{k}: {flatten_json(v, k)}\")\n",
    "    elif isinstance(obj, list):\n",
    "        parts.append(\", \".join([flatten_json(x, prefix) for x in obj]))\n",
    "    else:\n",
    "        parts.append(str(obj))\n",
    "    return \" \".join(str(p) for p in parts if p is not None)\n",
    "\n",
    "def map_hex_to_color_words(hex_code: str):\n",
    "    try:\n",
    "        if not hex_code.startswith(\"#\") or len(hex_code) != 7:\n",
    "            return []\n",
    "        r = int(hex_code[1:3], 16)/255.0\n",
    "        g = int(hex_code[3:5], 16)/255.0\n",
    "        b = int(hex_code[5:7], 16)/255.0\n",
    "        mx, mn = max(r,g,b), min(r,g,b)\n",
    "        d = mx - mn\n",
    "        if d == 0:\n",
    "            h = 0\n",
    "        elif mx == r:\n",
    "            h = (60 * ((g - b)/d) + 360) % 360\n",
    "        elif mx == g:\n",
    "            h = (60 * ((b - r)/d) + 120) % 360\n",
    "        else:\n",
    "            h = (60 * ((r - g)/d) + 240) % 360\n",
    "        if 170 <= h <= 250:\n",
    "            return [\"blue\", \"marine\"]\n",
    "        if 80 <= h <= 160:\n",
    "            return [\"green\"]\n",
    "        if 20 <= h <= 50:\n",
    "            return [\"gold\", \"amber\"]\n",
    "        if h < 20 or h > 300:\n",
    "            return [\"red\", \"warm\"]\n",
    "        if 250 < h <= 300:\n",
    "            return [\"violet\"]\n",
    "        return []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "FAMILY_TO_ROLE = {\n",
    "    \"citrus\": \"top\", \"green\": \"top\", \"aldehydic\": \"top\", \"marine\": \"top\",\n",
    "    \"lactonic\": \"mid\", \"floral\": \"mid\", \"spicy\": \"mid\", \"herbal\": \"mid\",\n",
    "    \"woody\": \"base\", \"amber\": \"base\", \"musk\": \"base\", \"leathery\": \"base\",\n",
    "    \"oud\": \"base\", \"balsamic\": \"base\", \"incense\": \"base\",\n",
    "}\n",
    "\n",
    "def pick_role_from_family(family: str):\n",
    "    if not family:\n",
    "        return \"mid\"\n",
    "    fam = normalize_text(family).replace(\"/\", \" \").split()[0]\n",
    "    return FAMILY_TO_ROLE.get(fam, \"mid\")\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- data_ingest/build_collections.py ----------------\n",
    "w(INGEST/\"build_collections.py\", r\"\"\"\n",
    "import os, json, pickle, pandas as pd\n",
    "from pathlib import Path\n",
    "from agent.local_utils import TinyTfidf, normalize_text, flatten_json\n",
    "\n",
    "BASE = Path(__file__).resolve().parents[1]\n",
    "INDICES = BASE / \"indices\"\n",
    "\n",
    "# Data directory (put your four files here)\n",
    "DATA_DIR = Path(os.environ.get(\"LUNAR_DATA_DIR\", \"data\"))\n",
    "MATERIALS = DATA_DIR/\"materials_catalog.jsonl\"\n",
    "FORMULAS  = DATA_DIR/\"formulas.jsonl\"\n",
    "NOTES1    = DATA_DIR/\"nlp_notes_data.csv\"\n",
    "NOTES2    = DATA_DIR/\"fra_cleaned.csv\"\n",
    "\n",
    "def read_jsonl(path: Path):\n",
    "    items = []\n",
    "    if not path.exists():\n",
    "        return items\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                items.append(json.loads(line))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    items.append(json.loads(line.rstrip(\", \")))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return items\n",
    "\n",
    "def extract_material_record(raw):\n",
    "    name = raw.get(\"material_name\") or raw.get(\"name\") or raw.get(\"material\") or raw.get(\"Name\") or raw.get(\"Material\")\n",
    "    if not name:\n",
    "        for _,v in raw.items():\n",
    "            if isinstance(v, str) and len(v) > 1:\n",
    "                name = v; break\n",
    "    family = raw.get(\"family\") or raw.get(\"Family\") or raw.get(\"olfactive_family\") or raw.get(\"Olfactive family\")\n",
    "    descriptors = raw.get(\"descriptors\") or raw.get(\"description\") or raw.get(\"notes\") or raw.get(\"Descriptors\")\n",
    "    tenacity = raw.get(\"tenacity\") or raw.get(\"Tenacity\")\n",
    "    usage_max = raw.get(\"usage_max\") or raw.get(\"ifra_max\") or raw.get(\"max_usage\")\n",
    "    allergens = raw.get(\"allergens\") or raw.get(\"Allergens\")\n",
    "    potency = raw.get(\"strength\") or raw.get(\"potency\") or raw.get(\"intensity\")\n",
    "    text_blob = \" ; \".join([\n",
    "        str(name or \"\"),\n",
    "        f\"family={family}\" if family else \"\",\n",
    "        f\"tenacity={tenacity}\" if tenacity else \"\",\n",
    "        f\"potency={potency}\" if potency else \"\",\n",
    "        f\"descriptors={descriptors}\" if descriptors else \"\",\n",
    "        f\"usage_max={usage_max}\" if usage_max else \"\",\n",
    "        f\"allergens={allergens}\" if allergens else \"\",\n",
    "    ])\n",
    "    return {\n",
    "        \"name\": name or \"unknown-material\",\n",
    "        \"family\": family,\n",
    "        \"descriptors\": descriptors,\n",
    "        \"tenacity\": tenacity,\n",
    "        \"potency\": potency,\n",
    "        \"usage_max\": usage_max,\n",
    "        \"allergens\": allergens,\n",
    "        \"text\": text_blob\n",
    "    }\n",
    "\n",
    "def load_notes_csv(path: Path):\n",
    "    rows = []\n",
    "    if not path.exists():\n",
    "        return rows\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "    name_candidates = [c for c in cols if \"note\" in c.lower() or \"name\" in c.lower() or c.lower() in (\"material\",\"ingredient\")]\n",
    "    family_candidates = [c for c in cols if \"family\" in c.lower()]\n",
    "    desc_candidates = [c for c in cols if \"desc\" in c.lower() or \"descriptor\" in c.lower() or \"info\" in c.lower() or \"text\" in c.lower() or \"synopsis\" in c.lower()]\n",
    "    name_col = name_candidates[0] if name_candidates else cols[0]\n",
    "    fam_col  = family_candidates[0] if family_candidates else None\n",
    "    des_col  = desc_candidates[0] if desc_candidates else None\n",
    "    for _, r in df.iterrows():\n",
    "        name = str(r.get(name_col, \"\")).strip()\n",
    "        if not name: \n",
    "            continue\n",
    "        family = str(r.get(fam_col, \"\")).strip() if fam_col else \"\"\n",
    "        desc = str(r.get(des_col, \"\")).strip() if des_col else \"\"\n",
    "        text = \" ; \".join([name, f\"family={family}\" if family else \"\", desc])\n",
    "        rows.append({\"note\": name, \"family\": family, \"description\": desc, \"text\": text})\n",
    "    return rows\n",
    "\n",
    "def main():\n",
    "    INDICES.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Materials\n",
    "    materials_raw = read_jsonl(MATERIALS)\n",
    "    materials = [extract_material_record(x) for x in materials_raw]\n",
    "    materials_corpus = [x[\"text\"] for x in materials] if materials else [\"placeholder\"]\n",
    "    mat_vec = TinyTfidf().fit(materials_corpus)\n",
    "    mat_X = mat_vec.transform(materials_corpus)\n",
    "    pickle.dump({\"vectorizer\": mat_vec, \"X\": mat_X, \"rows\": materials}, open(INDICES/\"materials.pkl\",\"wb\"))\n",
    "\n",
    "    # Notes\n",
    "    notes = load_notes_csv(NOTES1) + load_notes_csv(NOTES2)\n",
    "    notes_corpus = [x[\"text\"] for x in notes] if notes else [\"placeholder\"]\n",
    "    note_vec = TinyTfidf().fit(notes_corpus)\n",
    "    note_X = note_vec.transform(notes_corpus)\n",
    "    pickle.dump({\"vectorizer\": note_vec, \"X\": note_X, \"rows\": notes}, open(INDICES/\"notes.pkl\",\"wb\"))\n",
    "\n",
    "    # Formulas\n",
    "    formulas_raw = read_jsonl(FORMULAS)\n",
    "    formulas_rows = []\n",
    "    for fr in formulas_raw:\n",
    "        text = flatten_json(fr)\n",
    "        formulas_rows.append({\"raw\": fr, \"text\": text})\n",
    "    formulas_corpus = [x[\"text\"] for x in formulas_rows] if formulas_rows else [\n",
    "        \"top bergamot 2 aldehydes 0.3 mid rose 4 jasmine 2 base cedar 3 amber 2 musk 0.5\"\n",
    "    ]\n",
    "    form_vec = TinyTfidf().fit(formulas_corpus)\n",
    "    form_X = form_vec.transform(formulas_corpus)\n",
    "    pickle.dump({\"vectorizer\": form_vec, \"X\": form_X, \"rows\": formulas_rows}, open(INDICES/\"formulas.pkl\",\"wb\"))\n",
    "\n",
    "    # Global vectorizer (for evaluator)\n",
    "    global_corpus = materials_corpus + notes_corpus + formulas_corpus + [\n",
    "        \"calm airy sea breeze marine tranquil luminous confident polished green fresh clean warm woody leathery musk amber balsamic incense citrus floral spicy herbal aldehydic lactonic\"\n",
    "    ]\n",
    "    glob_vec = TinyTfidf().fit(global_corpus)\n",
    "    pickle.dump({\"vectorizer\": glob_vec}, open(INDICES/\"global.pkl\",\"wb\"))\n",
    "\n",
    "    print(\"Indices built in\", INDICES)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- agent/intent_parser.py ----------------\n",
    "w(AGENT/\"intent_parser.py\", r\"\"\"\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from agent.local_utils import normalize_text\n",
    "\n",
    "SEASONS = [\"spring\",\"summer\",\"autumn\",\"fall\",\"winter\",\"all-season\"]\n",
    "CONTEXTS = [\"spa\",\"gym\",\"office\",\"hotel\",\"retail\",\"boutique\",\"menswear\",\"womenswear\",\"wellness\",\"lobby\",\"app\",\"lounge\",\"event\"]\n",
    "\n",
    "@dataclass\n",
    "class Intent:\n",
    "    season: Optional[str]\n",
    "    context: Optional[str]\n",
    "    target_audience: Optional[str]\n",
    "    brand_tone: List[str]\n",
    "    constraints: List[str]\n",
    "    color_terms: List[str]\n",
    "    raw: str\n",
    "\n",
    "def parse_colors(raw: str, extra_colors: List[str]) -> List[str]:\n",
    "    color_words = []\n",
    "    color_words += re.findall(r\"#(?:[0-9a-fA-F]{6})\", raw)\n",
    "    named = re.findall(r\"\\b(black|white|gold|amber|teal|green|blue|red|violet|purple|pink|sand|beige)\\b\", raw)\n",
    "    color_words += [c.lower() for c in named]\n",
    "    color_words += [c.lower() for c in extra_colors if c]\n",
    "    seen = set(); out = []\n",
    "    for c in color_words:\n",
    "        if c not in seen:\n",
    "            out.append(c); seen.add(c)\n",
    "    return out\n",
    "\n",
    "def parse_intent(brief: str, colors: List[str]) -> Intent:\n",
    "    txt = normalize_text(brief)\n",
    "    season = next((s for s in SEASONS if s in txt), None)\n",
    "    context = next((c for c in CONTEXTS if c in txt), None)\n",
    "    target = None\n",
    "    if \"menswear\" in txt or \" men \" in f\" {txt} \":\n",
    "        target = \"men\"\n",
    "    elif \"womenswear\" in txt or \" women \" in f\" {txt} \":\n",
    "        target = \"women\"\n",
    "    tone = [w for w in txt.split() if w.endswith(\"y\") or w.endswith(\"al\") or w in (\"calm\",\"fresh\",\"clean\",\"confident\",\"polished\",\"minimalist\",\"uplifting\",\"warm\",\"airy\",\"marine\")]\n",
    "    constraints = []\n",
    "    if \"avoid animalic\" in txt or \"no animalic\" in txt:\n",
    "        constraints.append(\"no_animalic\")\n",
    "    color_terms = parse_colors(txt, colors)\n",
    "    return Intent(season, context, target, tone, constraints, color_terms, raw=brief)\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- agent/sensory_moodboard.py ----------------\n",
    "w(AGENT/\"sensory_moodboard.py\", r\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from agent.local_utils import map_hex_to_color_words, normalize_text\n",
    "\n",
    "@dataclass\n",
    "class SensorySeed:\n",
    "    seed_notes: List[str]\n",
    "    emotion_text: str\n",
    "    emotion_terms: List[str]\n",
    "\n",
    "COLOR_HINTS = {\n",
    "    \"teal\": [\"marine\",\"green\",\"fresh\"],\n",
    "    \"gold\": [\"amber\",\"warm\",\"luminous\"],\n",
    "    \"black\": [\"leathery\",\"smoky\",\"dark\"],\n",
    "    \"white\": [\"clean\",\"airy\",\"minimalist\"],\n",
    "    \"green\": [\"green\",\"herbal\",\"fresh\"],\n",
    "    \"blue\": [\"marine\",\"cool\",\"calm\"],\n",
    "    \"sand\": [\"amber\",\"beige\",\"soft\"],\n",
    "    \"amber\": [\"amber\",\"warm\"],\n",
    "}\n",
    "\n",
    "def build_seed(intent_tone: List[str], color_terms: List[str]) -> SensorySeed:\n",
    "    hints = []\n",
    "    for c in color_terms:\n",
    "        if c.startswith(\"#\"):\n",
    "            hints += map_hex_to_color_words(c)\n",
    "        else:\n",
    "            hints += COLOR_HINTS.get(c, [])\n",
    "    uniq = []\n",
    "    for w in (hints + intent_tone):\n",
    "        w = normalize_text(w)\n",
    "        if w and w not in uniq:\n",
    "            uniq.append(w)\n",
    "    base_emotions = [w for w in uniq if w in (\"calm\",\"fresh\",\"clean\",\"warm\",\"airy\",\"marine\",\"luminous\",\"minimalist\",\"uplifting\",\"confident\",\"polished\",\"soft\",\"cool\",\"dark\")]\n",
    "    emotion_text = \" \".join(base_emotions[:6]) or \"balanced modern clean\"\n",
    "    return SensorySeed(seed_notes=uniq[:12], emotion_text=emotion_text, emotion_terms=uniq)\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- agent/note_rag.py ----------------\n",
    "w(AGENT/\"note_rag.py\", r\"\"\"\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from agent.local_utils import cosine, normalize_text\n",
    "\n",
    "BASE = Path(__file__).resolve().parents[1]\n",
    "INDICES = BASE / \"indices\"\n",
    "\n",
    "@dataclass\n",
    "class Candidate:\n",
    "    name: str\n",
    "    family: str\n",
    "    meta: Dict\n",
    "    score: float\n",
    "\n",
    "def _top_k(vec, X, rows, k=20):\n",
    "    sims = [(i, cosine(vec, x)) for i, x in enumerate(X)]\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    out = []\n",
    "    for i, s in sims[:k]:\n",
    "        r = rows[i]\n",
    "        name = r.get(\"name\") or r.get(\"note\") or r.get(\"text\",\"\")[:40]\n",
    "        family = r.get(\"family\") or \"\"\n",
    "        out.append(Candidate(name=name, family=family, meta=r, score=float(s)))\n",
    "    return out\n",
    "\n",
    "def retrieve_candidates(seed_terms: List[str], emotion_text: str, top_k=24):\n",
    "    with open(INDICES/\"notes.pkl\", \"rb\") as f:\n",
    "        notes = pickle.load(f)\n",
    "    with open(INDICES/\"materials.pkl\", \"rb\") as f:\n",
    "        mats = pickle.load(f)\n",
    "    query = \" \".join(seed_terms + [emotion_text])\n",
    "    q_notes = notes[\"vectorizer\"].transform([query])[0]\n",
    "    q_mats  = mats[\"vectorizer\"].transform([query])[0]\n",
    "    cand_notes = _top_k(q_notes, notes[\"X\"], notes[\"rows\"], k=top_k//2)\n",
    "    cand_mats  = _top_k(q_mats,  mats[\"X\"],  mats[\"rows\"],  k=top_k//2)\n",
    "    merged = {}\n",
    "    for c in cand_notes + cand_mats:\n",
    "        key = normalize_text(c.name)\n",
    "        if key not in merged or c.score > merged[key].score:\n",
    "            merged[key] = c\n",
    "    return sorted(merged.values(), key=lambda x: x.score, reverse=True)\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- agent/scent_composer.py ----------------\n",
    "w(AGENT/\"scent_composer.py\", r\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from agent.local_utils import pick_role_from_family\n",
    "\n",
    "@dataclass\n",
    "class MaterialUse:\n",
    "    name: str\n",
    "    role: str\n",
    "    percent: float\n",
    "    info: Dict\n",
    "\n",
    "@dataclass\n",
    "class Formula:\n",
    "    top: List[MaterialUse]\n",
    "    mid: List[MaterialUse]\n",
    "    base: List[MaterialUse]\n",
    "\n",
    "def allocate_roles(candidates, max_per_role=5):\n",
    "    buckets = {\"top\":[], \"mid\":[], \"base\":[]}\n",
    "    for c in candidates:\n",
    "        fam = c.family or (c.meta.get(\"family\") if isinstance(c.meta, dict) else \"\")\n",
    "        role = pick_role_from_family(fam)\n",
    "        buckets[role].append(c)\n",
    "    # backfill if empty\n",
    "    allc = candidates[:]\n",
    "    for r in (\"top\",\"mid\",\"base\"):\n",
    "        if not buckets[r] and allc:\n",
    "            buckets[r].append(allc[0])\n",
    "    for r in buckets:\n",
    "        buckets[r] = buckets[r][:max_per_role]\n",
    "    return buckets\n",
    "\n",
    "def percent_plan(role_counts):\n",
    "    split = {\"top\":0.30, \"mid\":0.40, \"base\":0.30}\n",
    "    plan = {}\n",
    "    for r, cnt in role_counts.items():\n",
    "        if cnt <= 0:\n",
    "            plan[r] = []\n",
    "            continue\n",
    "        share = split[r]*100.0\n",
    "        weights = [1.0/(i+1) for i in range(cnt)]\n",
    "        s = sum(weights)\n",
    "        plan[r] = [round(share*(w/s), 2) for w in weights]\n",
    "    return plan\n",
    "\n",
    "def compose(candidates) -> Formula:\n",
    "    buckets = allocate_roles(candidates)\n",
    "    plan = percent_plan({r:len(buckets[r]) for r in buckets})\n",
    "    def mk(role):\n",
    "        out = []\n",
    "        for i, c in enumerate(buckets[role]):\n",
    "            out.append(MaterialUse(\n",
    "                name=c.name, role=role, percent=plan[role][i] if i < len(plan[role]) else 0.0, info=c.meta\n",
    "            ))\n",
    "        return out\n",
    "    return Formula(top=mk(\"top\"), mid=mk(\"mid\"), base=mk(\"base\"))\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- agent/compliance_agent.py ----------------\n",
    "w(AGENT/\"compliance_agent.py\", r\"\"\"\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass\n",
    "class ComplianceResult:\n",
    "    ok: bool\n",
    "    warnings: List[str]\n",
    "    fixes: List[Dict]\n",
    "\n",
    "def check(formula):\n",
    "    warnings = []\n",
    "    fixes = []\n",
    "    for role_list in [formula.top, formula.mid, formula.base]:\n",
    "        for mu in role_list:\n",
    "            meta = mu.info if isinstance(mu.info, dict) else {}\n",
    "            usage_max = meta.get(\"usage_max\")\n",
    "            try:\n",
    "                if usage_max is not None:\n",
    "                    if isinstance(usage_max, str) and \"%\" in usage_max:\n",
    "                        um = float(usage_max.replace(\"%\",\"\").strip())\n",
    "                    else:\n",
    "                        um = float(usage_max)\n",
    "                    if mu.percent > um:\n",
    "                        fixes.append({\"material\": mu.name, \"from\": mu.percent, \"to\": um, \"reason\": \"usage_max clamp\"})\n",
    "                if meta.get(\"allergens\"):\n",
    "                    warnings.append(f\"{mu.name}: allergens listed: {meta.get('allergens')}\")\n",
    "            except Exception:\n",
    "                continue\n",
    "    ok = len([fx for fx in fixes if fx[\"to\"] < fx[\"from\"]]) == 0\n",
    "    return ComplianceResult(ok=ok, warnings=warnings, fixes=fixes)\n",
    "\n",
    "def apply_fixes(formula, fixes):\n",
    "    target = {fx[\"material\"]: fx[\"to\"] for fx in fixes if fx.get(\"to\") is not None}\n",
    "    if not target:\n",
    "        return formula\n",
    "    all_items = formula.top + formula.mid + formula.base\n",
    "    for mu in all_items:\n",
    "        if mu.name in target:\n",
    "            mu.percent = target[mu.name]\n",
    "    total = sum(mu.percent for mu in all_items) or 1.0\n",
    "    for mu in all_items:\n",
    "        mu.percent = round(100.0 * mu.percent / total, 2)\n",
    "    return formula\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- agent/evaluator.py ----------------\n",
    "w(AGENT/\"evaluator.py\", r\"\"\"\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(__file__).resolve().parents[1]\n",
    "INDICES = BASE / \"indices\"\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    score: float\n",
    "    rationale: str\n",
    "\n",
    "def formula_to_text(formula):\n",
    "    def part(lst):\n",
    "        return \", \".join([f\"{x.name} {x.percent}%\" for x in lst])\n",
    "    return f\"top: {part(formula.top)} | mid: {part(formula.mid)} | base: {part(formula.base)}\"\n",
    "\n",
    "def evaluate(formula, emotion_text: str) -> EvalResult:\n",
    "    with open(INDICES/\"global.pkl\", \"rb\") as f:\n",
    "        glob = pickle.load(f)\n",
    "    vec = glob[\"vectorizer\"]\n",
    "    ftxt = formula_to_text(formula)\n",
    "    q = vec.transform([emotion_text])[0]\n",
    "    d = vec.transform([ftxt])[0]\n",
    "    score = sum(a*b for a,b in zip(q,d))\n",
    "    s = max(0.0, min(1.0, score))\n",
    "    scaled = round(40.0 + 55.0 * s, 1)\n",
    "    rationale = \"Alignment via TF-IDF cosine on local corpus.\"\n",
    "    return EvalResult(score=scaled, rationale=rationale)\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- agent/brand_styler.py ----------------\n",
    "w(AGENT/\"brand_styler.py\", r\"\"\"\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Branding:\n",
    "    name: str\n",
    "    story: str\n",
    "\n",
    "ADJ = [\"Lunar\",\"Equinox\",\"Nocturne\",\"Aurea\",\"Aqua\",\"Verde\",\"Obsidian\",\"Nimbus\",\"Serene\",\"Solstice\"]\n",
    "NUUN = [\"Accord\",\"Veil\",\"Current\",\"Silhouette\",\"Tide\",\"Ember\",\"Whisper\",\"Halo\",\"Pulse\",\"Drift\"]\n",
    "\n",
    "def style(intent, formula) -> Branding:\n",
    "    base = random.choice(ADJ) + \" \" + random.choice(NUUN)\n",
    "    tone = \", \".join(list(dict.fromkeys(intent.brand_tone[:4]))) or \"modern, clean\"\n",
    "    line = f\"{base} captures a {tone} aura, translating your brief into an elegant balance of top, heart, and base.\"\n",
    "    return Branding(name=base, story=line)\n",
    "\"\"\")\n",
    "\n",
    "# ---------------- agent/orchestrator.py ----------------\n",
    "w(AGENT/\"orchestrator.py\", r\"\"\"\n",
    "from pathlib import Path\n",
    "from agent.intent_parser import parse_intent\n",
    "from agent.sensory_moodboard import build_seed\n",
    "from agent.note_rag import retrieve_candidates\n",
    "from agent.scent_composer import compose\n",
    "from agent.compliance_agent import check, apply_fixes\n",
    "from agent.evaluator import evaluate\n",
    "from agent.brand_styler import style\n",
    "\n",
    "BASE = Path(__file__).resolve().parents[1]\n",
    "OUTPUTS = (BASE / \"examples\" / \"outputs\")\n",
    "OUTPUTS.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def generate(brief: str, colors=None):\n",
    "    colors = colors or []\n",
    "    intent = parse_intent(brief, colors)\n",
    "    seed = build_seed(intent.brand_tone, intent.color_terms)\n",
    "    cands = retrieve_candidates(seed.seed_notes, seed.emotion_text, top_k=24)\n",
    "    formula = compose(cands)\n",
    "    comp = check(formula)\n",
    "    if comp.fixes:\n",
    "        formula = apply_fixes(formula, comp.fixes)\n",
    "    evalr = evaluate(formula, seed.emotion_text)\n",
    "    brand = style(intent, formula)\n",
    "    res = {\n",
    "        \"intent\": vars(intent),\n",
    "        \"seed\": {\"seed_notes\": seed.seed_notes, \"emotion_text\": seed.emotion_text},\n",
    "        \"formula\": {\n",
    "            \"top\": [vars(x) for x in formula.top],\n",
    "            \"mid\": [vars(x) for x in formula.mid],\n",
    "            \"base\": [vars(x) for x in formula.base],\n",
    "        },\n",
    "        \"compliance\": {\n",
    "            \"ok\": comp.ok,\n",
    "            \"warnings\": comp.warnings,\n",
    "            \"fixes\": comp.fixes,\n",
    "        },\n",
    "        \"evaluation\": {\"score\": evalr.score, \"rationale\": evalr.rationale},\n",
    "        \"branding\": {\"name\": brand.name, \"story\": brand.story},\n",
    "    }\n",
    "    return res\n",
    "\n",
    "def save_output(res: dict, name: str):\n",
    "    import json\n",
    "    out_json = OUTPUTS / f\"{name}.json\"\n",
    "    out_md   = OUTPUTS / f\"{name}.md\"\n",
    "    out_json.write_text(json.dumps(res, indent=2, ensure_ascii=False))\n",
    "    def part(lst):\n",
    "        return \", \".join([f\"{x['name']} {x['percent']}%\" for x in lst])\n",
    "    md = (\n",
    "    \"# Lunar AIccord — {name}\\n\\n\"\n",
    "    \"**Score:** {score} / 100  \\n\"\n",
    "    \"**Story:** {story}\\n\\n\"\n",
    "    \"## Formula\\n\"\n",
    "    \"- **Top:** {top}\\n\"\n",
    "    \"- **Mid:** {mid}\\n\"\n",
    "    \"- **Base:** {base}\\n\\n\"\n",
    "    \"## Compliance\\n\"\n",
    "    \"- OK: {ok}\\n\"\n",
    "    \"- Warnings: {warnings}\\n\"\n",
    "    \"- Fixes: {fixes}\\n\"\n",
    "    ).format(\n",
    "        name=res['branding']['name'],\n",
    "        score=res['evaluation']['score'],\n",
    "        story=res['branding']['story'],\n",
    "        top=part(res['formula']['top']),\n",
    "        mid=part(res['formula']['mid']),\n",
    "        base=part(res['formula']['base']),\n",
    "        ok=res['compliance']['ok'],\n",
    "        warnings=\"; \".join(res['compliance']['warnings']) or \"None\",\n",
    "        fixes=res['compliance']['fixes'] or \"None\",\n",
    "    )\n",
    "\n",
    "    out_md.write_text(md)\n",
    "    return str(out_json), str(out_md)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ---------------- run.py ----------------\n",
    "w(BASE/\"run.py\", r\"\"\"\n",
    "import argparse, os\n",
    "from pathlib import Path\n",
    "from agent.orchestrator import generate, save_output\n",
    "from data_ingest.build_collections import main as build_indices\n",
    "\n",
    "def parse_colors_arg(s: str):\n",
    "    if not s:\n",
    "        return []\n",
    "    parts = [x.strip() for x in s.split(\",\")]\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--brief\", required=True, help=\"Path to a text file containing the brief\")\n",
    "    ap.add_argument(\"--colors\", default=\"\", help=\"Comma-separated color terms or hex (e.g., \\\"#2a5caa,gold\\\")\")\n",
    "    ap.add_argument(\"--data_dir\", default=\"data\", help=\"Folder containing the four datasets\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    # Make indices (uses args.data_dir)\n",
    "    os.environ[\"LUNAR_DATA_DIR\"] = args.data_dir\n",
    "\n",
    "    brief_path = Path(args.brief)\n",
    "    brief = brief_path.read_text(encoding=\"utf-8\")\n",
    "    colors = parse_colors_arg(args.colors)\n",
    "\n",
    "    build_indices()\n",
    "    res = generate(brief, colors)\n",
    "    jpath, mpath = save_output(res, brief_path.stem)\n",
    "    print(f\"Saved:\\n  JSON: {jpath}\\n  MD:   {mpath}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Wrote files to\", BASE)\n",
    "print(\"Now run Cell 2 to build indices and Cell 3 for a demo.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
